import copy
import math
import torch
import torch.nn as nn

class LCG(nn.Module):
    def __init__(self, vis_encoder, embed_dim):
        super().__init__()
        self.transformer_layer = copy.deepcopy(vis_encoder.transformer.resblocks[-1])
        self.local_transformer = nn.Sequential(self.transformer_layer)
        self.global_transformer = nn.Sequential(self.transformer_layer,
                                                self.transformer_layer)  # todo 可拓展层
        # self.LCG_transformer = nn.Sequential(self.transformer_layer)

        self.divide_length = 3
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # 全局特征
        self.part_token1 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.part_token2 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.part_token3 = nn.Parameter(torch.zeros(1, 1, embed_dim))

        trunc_normal_(self.part_token1, std=.02)
        trunc_normal_(self.part_token2, std=.02)
        trunc_normal_(self.part_token3, std=.02)



    def forward(self, features):
        B = features.shape[0]
        part_token1 = self.part_token1.expand(B, -1, -1)
        part_token2 = self.part_token2.expand(B, -1, -1)
        part_token3 = self.part_token3.expand(B, -1, -1)

        feature_length = features.size(1) - 1
        patch_length = feature_length // self.divide_length
        token = features[:, 0:1]
        x = features[:, 1:]
        # lf_1
        b1_local_feat = x[:, :patch_length]
        b1_local_feat = self.local_transformer(torch.cat((part_token1, b1_local_feat), dim=1))
        local_feat_1 = b1_local_feat[:, 0:1]

        # lf_2
        b2_local_feat = x[:, patch_length:patch_length*2]
        b2_local_feat = self.local_transformer(torch.cat((part_token2, b2_local_feat), dim=1))
        local_feat_2 = b2_local_feat[:, 0:1]

        # lf_3
        b3_local_feat = x[:, patch_length*2:]
        b3_local_feat = self.local_transformer(torch.cat((part_token3, b3_local_feat), dim=1))
        local_feat_3 = b3_local_feat[:, 0:1]

        # # lf_4
        # b4_local_feat = x[:, patch_length*3:patch_length*4]
        # b4_local_feat = self.local_transformer(torch.cat((part_token4, b4_local_feat), dim=1))
        # local_feat_4 = b4_local_feat[:, 0:1]
        # cls_tokens = self.cls_token.expand(B, -1, -1)

        # lcg_feats = self.LCG_transformer(torch.cat([cls_tokens, local_feat_1, local_feat_2, local_feat_3], dim=1))   # 三者这之间进行交互
        # lcg_feat = lcg_feats[:, 0:1]

        global_feats = self.global_transformer(torch.cat([token, local_feat_1, local_feat_2, local_feat_3, x], dim=1))  # [全局，局部， x]  用局部特征引导全局特征
        global_feat = global_feats[:, 0]

        return global_feat, [local_feat_1.squeeze(1), local_feat_2.squeeze(1), local_feat_3.squeeze(1)]  # todo 是否进入测试







def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    ## type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
              "The distribution of values may be incorrect.", )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor